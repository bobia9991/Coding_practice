{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versions\n",
    "\n",
    "The Software Source Code (SSC) used in the Proteomes Journal article to which the present Jupyter Notebook refers to is written in **Python 3** (3.7.6). The main Python modules used along the present SSC are:\n",
    "+ pandas (1.2.1)\n",
    "+ numpy (1.19.2)\n",
    "+ Bio (1.76)\n",
    "+ sklearn (0.22.1)\n",
    "+ bokeh (2.2.3)\n",
    "+ plotnine (0.7.1)\n",
    "+ seaborn (0.11.1)\n",
    "\n",
    "In addition, we also used some supporting Python modules:\n",
    "+ os (-)\n",
    "+ re (2.2.1)\n",
    "+ sys (-)\n",
    "+ math (-)\n",
    "+ random (-)\n",
    "\n",
    "# Style\n",
    "\n",
    "We tried to write this SSC as readable and self explanatory as we could. You should find a very brief comment-line preceding (almost) each code-line. We are aware that this could be too verbose for ultra-advanced Python coders, but our intention is to make this SSC as accessible as possible. In any case, if you have problems when re-using this SSC (or when adapting it to your project), please don't hesitate to contact any corresponding author of the Proteomes Journal article.\n",
    "\n",
    "# Acronyms\n",
    "\n",
    "+ **PrSM**: Proteoform Spectrum Match.\n",
    "+ **PTM**: Post-Translational Modification.\n",
    "+ **PS**: ProSight PD.\n",
    "+ **TP**: TopPIC Suite.\n",
    "\n",
    "# SSC\n",
    "\n",
    "We have organized this SSC in 19 major sections. In any case, you should be able to run the whole script in a \"sequence shot\" fashion just by doing `Run`, `Run All Cells` on the Jupyter Notebook bar.\n",
    "\n",
    "* **Part 0** - Preamble\n",
    "* **Part 1** - Loading ProSight and TopPIC data\n",
    "* **Part 2** - Harmonizing ProSight and TopPIC data\n",
    "* **Part 3** - Data wrangling\n",
    "* **Part 4** - DBSCAN parameters optimization with Bokeh\n",
    "* **Part 5** - Filtering PrSMs by intensity and retention time\n",
    "* **Part 6** - DBSCAN\n",
    "* **Part 7** - Cluster annotation (automatic phase)\n",
    "* **Part 8** - Cluster annotation (manual phase)\n",
    "* **Part 9** - Reproducing **Table 1**\n",
    "* **Part 10** - Reproducing **Figure 3**\n",
    "* **Part 11** - Reproducing **Supplementary Figure 1**\n",
    "* **Part 12** - Reproducing **Supplementary Figure 2**\n",
    "* **Part 13** - Reproducing **Supplementary Figure 3**\n",
    "* **Part 14** - LandScape plots preparation\n",
    "* **Part 15** - Reproducing **Figure 4** (LandScapes)\n",
    "* **Part 16** - Ratio plots preparation\n",
    "* **Part 17** - Reproducing **Figure 4** (Ratios)\n",
    "* **Part 18** - PTMs localization plots preparation\n",
    "* **Part 19** - Reproducing **Figure 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 - Preamble\n",
    "\n",
    "In this part we import all Python modules we need (be sure you have all these modules installed in your machine before going downstream) and some \"User Defined Functions\" that we prepared in a separate file called `Proteomes_engine.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing supporting packages\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Importing pandas and numpy packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing plotnine and seaborn plotting packages\n",
    "import seaborn as sns\n",
    "import plotnine as p9\n",
    "\n",
    "# Importing plotnine tools explicitely in order to save code wording \n",
    "from plotnine import ggplot, aes, ggtitle, xlab, ylab, xlim, ylim, facet_wrap, facet_grid, guides, guide_legend, element_text, element_blank\n",
    "from plotnine import scale_size, scale_fill_manual, scale_color_manual, position_jitterdodge, position_dodge2\n",
    "from plotnine import geom_point, geom_bar, geom_col, geom_errorbar, geom_boxplot, geom_rect, theme_bw, theme\n",
    "# Writing just \"ggplot\" is better than \"p9.ggplot\"\n",
    "\n",
    "# Importing biopython tools\n",
    "from Bio.SeqUtils import molecular_weight\n",
    "\n",
    "# Importing all UDFs within Proteomes_engine\n",
    "import Proteomes_engine as sperm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a couple of dictionaries with some variables which particular values should be tunned when reusing this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining experimental design dictionary: e\n",
    "e = {\n",
    "     # Defining experimental design categories\n",
    "     0: 'BioRep', 1: 'TechRep'\n",
    "     }\n",
    "\n",
    "# Defining project dictionary: p\n",
    "p = {\n",
    "     # Defining DBSCAN hyperparameters\n",
    "     'Îµ':     0.0012,\n",
    "     'n_min': 10,\n",
    "\n",
    "     # Defining mass lower and upper limits to filter-in\n",
    "     'm_lower_lim': 6100,\n",
    "     'm_upper_lim': 18000,\n",
    "\n",
    "     # Defining z-score of log2 I lower and upper limits to filter-in\n",
    "     'z-i_lower_lim': -10,\n",
    "     'z-i_upper_lim': +10,\n",
    "\n",
    "     # Defining retention time lower and upper limits to filter-in\n",
    "     'rt_lower_lim': 0,\n",
    "     'rt_upper_lim': 42,\n",
    "    \n",
    "     # Defining PTM site minimum TP localization probability (%)\n",
    "    'PTM_TP_prob': 75,\n",
    "    \n",
    "     # Defining PTM site minimum PS C-Score\n",
    "    'PTM_PS_CScore': 40,\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a \"protein\" fasta dictionary with the four protamines (Protamine 1, Protamine 2 precursor, Protamine 2 precursor isoform 2 and Protamine 2 precursor isoform 3) full sequences. This dictionary will be used to get the Start and Stop positions of each PrSMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting Proteins FASTA (Determine start and stop positions)\n",
    "p_fasta = {\n",
    "           'P1':     'MARYRCCRSQSRSRYYRQRQRSRRRRRRSCQTRRRAMRCCRPRYRPRCRRH',\n",
    "           'pre-P2': 'MVRYRVRSLSERSHEVYRQQLHGQEQGHHGQEEQGLSPEHVEVYERTHGQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRGCRTRKRTCRRH',\n",
    "           'P2-2':   'MVRYRVRSLSERSHEVYRQQLHGQEQGHHGQEEQGLSPEHVEVYERTHGQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRESLGDPLNQNFLSQKAAEPGREHAEGTKLPGPLTPSWKLRKSRPKHQVRP',\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a \"proteoform\" fasta dictionary with all the potential unmodified protamine proteoforms. This dictionary will be used to compute the mass of each proteoform and this, in turn, will be used to estimate the experimental mass shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting Proteoforms FASTA (Annotate proteoforms and compute mass shifts)\n",
    "pf_seq_dict = {\n",
    "               'P1':                                                        'ARYRCCRSQSRSRYYRQRQRSRRRRRRSCQTRRRAMRCCRPRYRPRCRRH',\n",
    "               'pre-P2': 'VRYRVRSLSERSHEVYRQQLHGQEQGHHGQEEQGLSPEHVEVYERTHGQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRGCRTRKRTCRRH',\n",
    "               'HPI2':                       'HGQEQGHHGQEEQGLSPEHVEVYERTHGQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRGCRTRKRTCRRH',\n",
    "               'HPS1':                                   'QGLSPEHVEVYERTHGQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRGCRTRKRTCRRH',\n",
    "               'HPS2':                                      'SPEHVEVYERTHGQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRGCRTRKRTCRRH',\n",
    "               'HP4':                                               'ERTHGQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRGCRTRKRTCRRH',\n",
    "               'HP2':                                                'RTHGQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRGCRTRKRTCRRH',\n",
    "               'HP3':                                                   'GQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRGCRTRKRTCRRH',\n",
    "               'P2-2':   'VRYRVRSLSERSHEVYRQQLHGQEQGHHGQEEQGLSPEHVEVYERTHGQSHYRRRHCSRRRLHRIHRRQHRSCRRRKRRSCRHRRRHRRESLGDPLNQNFLSQKAAEPGREHAEGTKLPGPLTPSWKLRKSRPKHQVRP',\n",
    "               }\n",
    "\n",
    "# Initiating empty list to compute / store pf masses in incomming for loop\n",
    "mass_list = []\n",
    "\n",
    "# Computing and storing the pf masses of our pf FASTA dictionary\n",
    "for seq in pf_seq_dict.values():\n",
    "\n",
    "    # Computing the monoisotopic molecular weight of running pf sequence\n",
    "    mass = molecular_weight(seq, seq_type='protein', monoisotopic=True)\n",
    "\n",
    "    # Storing the monoisotopic molecular weight of running pf sequence\n",
    "    mass_list.append(mass)\n",
    "\n",
    "del seq, mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wet-lab protocol we used blocks all cysteine residues (C) present in our protamine proteoforms with carbamidomethylation (+57 Da). Thus we must correct the masses of our sequences according this fully carbamidomethylated state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing number of cysteines present in each sequence and multiplying by +57\n",
    "mass_cys_list = [seq.count('C') * 57.021464 for seq in pf_seq_dict.values()]\n",
    "\n",
    "# Correcting 'mass_l' by adding 'Cys_mass_l': corrected_mass_a\n",
    "mass_corrected_list = list(np.array(mass_list) + np.array(mass_cys_list))\n",
    "\n",
    "# Creating a dictionary with the mass of each protein\n",
    "pf_mass_dict = dict(zip(pf_seq_dict.keys(), mass_corrected_list))\n",
    "\n",
    "# Creating a dictionary with the protein name of each sequence\n",
    "seq_pf_dict = dict(zip(pf_seq_dict.values(), pf_seq_dict.keys()))\n",
    "\n",
    "del mass_list, mass_cys_list, mass_corrected_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Loading ProSight and TopPIC data\n",
    "\n",
    "Now we import the PrSM data just as provided by PS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty list for incomming importing\n",
    "PS_l = []\n",
    "\n",
    "# Reading ProSight file names\n",
    "PS_f = [f for f in os.listdir(f'{os.getcwd()}\\\\Input\\\\Ps') if '.zip' in f]\n",
    "\n",
    "# For each file name in ProSight files name list...\n",
    "for f in PS_f:\n",
    "\n",
    "    # ... read it as a DataFrame and...\n",
    "    PS = pd.read_csv(f'{os.getcwd()}\\\\Input\\\\Ps\\\\{f}', sep='\\t', low_memory=False)\n",
    "\n",
    "    # ... append it to the empty list\n",
    "    PS_l.append(PS)\n",
    "\n",
    "del f, PS, PS_f\n",
    "\n",
    "# Concatenating all ProSight DataFrames into a single ProSight DataFrame\n",
    "PS_df = pd.concat(PS_l, axis=0, ignore_index=True)\n",
    "del PS_l\n",
    "\n",
    "# Dropping columns that slow-down DataFrame inspection\n",
    "for col in ['External Top Down Displays', 'Fragment Map']:\n",
    "\n",
    "    try:\n",
    "        PS_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "del col\n",
    "\n",
    "# Masking the well ranked in PS_df\n",
    "rank0_Î¼ = PS_df['Search Engine Rank'] < 3  # Get only 1st & 2nd\n",
    "rank1_Î¼ = PS_df['Rank'] == 1  # Get only 1st\n",
    "\n",
    "# Masking good confidence in PS_df\n",
    "conf_Î¼ = PS_df['Confidence'].isin(['High', 'Medium'])\n",
    "\n",
    "# Filtering the well ranked and good confidence in PS_df\n",
    "PS_df = PS_df[rank0_Î¼ & rank1_Î¼ & conf_Î¼].copy()\n",
    "del rank0_Î¼, rank1_Î¼, conf_Î¼\n",
    "\n",
    "# Correcting the mass in PS_df\n",
    "PS_df['MH [Da]'] = PS_df['Mass [Da]'] - 1.007276466879\n",
    "\n",
    "# Adding a column with the 'Node'\n",
    "PS_df['Node'] = 'PS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the PrSM data just as provided by TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty list for incomming importing\n",
    "TP_l = []\n",
    "\n",
    "# Reading TopPIC file names\n",
    "TP_f = [f for f in os.listdir(f'{os.getcwd()}\\\\Input\\\\Tp') if '.zip' in f]\n",
    "\n",
    "# For each file name in TopPic files name list...\n",
    "for f in TP_f:\n",
    "\n",
    "    # ... read it as a DataFrame and...\n",
    "    TP = pd.read_csv(f'{os.getcwd()}\\\\Input\\\\Tp\\\\{f}', sep='\\t', header=26)\n",
    "\n",
    "    # ... append it\n",
    "    TP_l.append(TP)\n",
    "\n",
    "del f, TP, TP_f\n",
    "\n",
    "# Concatenating all TopPic DataFrames\n",
    "TP_df = pd.concat(TP_l, axis=0, ignore_index=True)\n",
    "del TP_l\n",
    "\n",
    "# Taking the file name from the file complete path\n",
    "TP_df['Data file name'] = TP_df['Data file name'].str.split('/').str[-1]\n",
    "\n",
    "# Adding a column with the 'Node'\n",
    "TP_df['Node'] = 'TP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Harmonizing ProSight and TopPIC data\n",
    "\n",
    "Before getting  the `'Start'` and `'Stop'` positions of each PrSM for PS and TP, we need to get a \"cleaned\" `'Sequence'` column for TP from the \"dirty\" `'Proteoform'` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing 'Proteoform' to get rid of the X. and .X residues and storing them in 'Sequence'\n",
    "TP_df['Sequence'] = TP_df['Proteoform'].apply(lambda i: re.findall(r'\\..*\\.', i)[0])\n",
    "\n",
    "# Initiating replacing dictionary to clean sequences in TP_df['Sequence']\n",
    "clean_dict = {\n",
    "              r'\\(':      '',\n",
    "              r'\\)':      '',\n",
    "              r'\\.':      '',\n",
    "              r'\\[.*?\\]': '',\n",
    "              }\n",
    "\n",
    "# Cleaning sequences in 'Sequence' column\n",
    "TP_df['Sequence'] = TP_df['Sequence'].replace(clean_dict, regex=True)\n",
    "\n",
    "del clean_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get a cleaned `'Sequence'` column for TP, we can get the `'Start'` and `'Stop'` positions of each PrSM leveraging our USDs called `start_finder` and `stop_finder` from `Proteomes_engine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating DataFrame list for incomming for loop\n",
    "df_l = [PS_df, TP_df]\n",
    "\n",
    "# For each DataFrame in df_l...\n",
    "for df in df_l:\n",
    "\n",
    "    # ... obtain 'Start' residue positions of each 'Sequence'\n",
    "    df['Start'] = df['Sequence'].apply(sperm.start_finder, args=(p_fasta, ))\n",
    "\n",
    "    # ... obtain 'Stop' residue positions of each 'Sequence'\n",
    "    df['Stop'] = df['Sequence'].apply(sperm.stop_finder, args=(p_fasta, ))\n",
    "\n",
    "del df, df_l, p_fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node PS gives $-\\log_{10} E$. Here we compute the $-\\log_{10}$ of the `'E-value'` column from TP in order to harmonize with PS. We also compute $-\\log_{10}$ of the `'Q-value'` column from PS because we will need it for the next code chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the -log10 of the E value (TP)\n",
    "TP_df['-log E'] = -np.log10(TP_df['E-value'])\n",
    "\n",
    "# Computing the -log10 of the Q value (PS)\n",
    "PS_df['-log Q'] = -np.log10(PS_df['Q-value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some duplicated scans in PS. We will ammend this by keeping those PrSMs with the biggest $-\\log_{10} E$ and, in case of a tie, the biggest $-\\log_{10} Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sorting and dropping lists\n",
    "sorting_l = ['-Log E-Value', '-log Q']\n",
    "dropping_l = ['Spectrum File', 'Fragmentation Scan(s)', 'Mass [Da]']\n",
    "\n",
    "# Sorting by E and Q (descending). We want the big ones first!\n",
    "PS_df.sort_values(by=sorting_l, ascending=False, inplace=True)\n",
    "\n",
    "# Dropping duplicates (keep first). We want the big ones first!\n",
    "PS_df.drop_duplicates(subset=dropping_l, keep='first', inplace=True)\n",
    "\n",
    "# Sorting by index to recover the \"original\" sorting again\n",
    "PS_df.sort_index()\n",
    "\n",
    "del sorting_l, dropping_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node has provides information regarding the PTM localization. Unfortunately, the format of this information is not the same for PS and TP. We need to parse their columns `'Modifications'` and `'MIScore'`. To this aim we coded a couple of (bulky) parser USDs called `PS_parser` and `TP_parser`. As usual, you can find the source code in `Proteomes_engine.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying user-defined fuction to parse 'Modifications' column from PS\n",
    "PS_df = sperm.PS_parser(PS_df)\n",
    "\n",
    "# Applying user-defined fuction to parse 'MIScore' column from TP\n",
    "TP_df = sperm.TP_parser(TP_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we also need to parse `'Modifications'` (PS) and `'Proteoform'` (TP) to get the number of phosphorylations on each PrSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing to get the number of phosphorylations\n",
    "PS_df['Count (ph)'] = PS_df['Modifications'].str.count(pat='phospho')\n",
    "TP_df['Count (ph)'] = TP_df['Proteoform'].str.count(pat='Phospho')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before merging `TP_df` and `PS_df` DataFrame, we need to construct the `'PF ID'` column. The output data from TP already gives this proteoform ID column, but we need to create it by hand for PS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuff(orig):\n",
    "    '''TODO'''\n",
    "    random.seed(2020)\n",
    "    dest = orig[:]\n",
    "    random.shuffle(dest)\n",
    "    return dest\n",
    "\n",
    "# Getting nicely fomatted proteoform ID column from PS\n",
    "PS_df['PF ID'] = PS_df['Annotated Sequence'] + PS_df['Modifications']\n",
    "PS_IDs = list(PS_df['PF ID'].unique())\n",
    "PS_nums = list(range(len(PS_IDs)))\n",
    "PS_nums_shuff = shuff(PS_nums)\n",
    "PS_ID_dict = dict(zip(PS_IDs, PS_nums_shuff))\n",
    "PS_df['Proteoform ID'] = PS_df['PF ID'].map(PS_ID_dict)\n",
    "PS_df['Proteoform ID'] = PS_df['Proteoform ID'].apply(int)\n",
    "PS_df['Proteoform ID'] = PS_df['Proteoform ID'].apply(str)\n",
    "PS_df['Proteoform ID'] = PS_df['Proteoform ID'].str.zfill(3)\n",
    "PS_df['Proteoform ID'] = PS_df['Node'] + '-' + PS_df['Proteoform ID']\n",
    "\n",
    "# Getting nicely fomatted proteoform ID column from TP\n",
    "TP_df['PF ID'] = TP_df['Proteoform ID'].apply(str)\n",
    "TP_IDs = list(TP_df['PF ID'].unique())\n",
    "TP_nums = list(range(len(TP_IDs)))\n",
    "TP_nums_shuff = shuff(TP_nums)\n",
    "TP_ID_dict = dict(zip(TP_IDs, TP_nums_shuff))\n",
    "TP_df['Proteoform ID'] = TP_df['PF ID'].map(TP_ID_dict)\n",
    "TP_df['Proteoform ID'] = TP_df['Proteoform ID'].apply(int)\n",
    "TP_df['Proteoform ID'] = TP_df['Proteoform ID'].apply(str)\n",
    "TP_df['Proteoform ID'] = TP_df['Proteoform ID'].str.zfill(3)\n",
    "TP_df['Proteoform ID'] = TP_df['Node'] + '-' + TP_df['Proteoform ID']\n",
    "\n",
    "del PS_IDs, PS_nums, PS_ID_dict, PS_nums_shuff\n",
    "del TP_IDs, TP_nums, TP_ID_dict, TP_nums_shuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in disposition to merge `TP_df` and `PS_df` DataFrames, but we must standardize column names first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing empty list for incomming column name standarization\n",
    "df_l = []\n",
    "\n",
    "# Standarizing column names in PS\n",
    "sperm.col_renamer(PS_df)\n",
    "df_l.append(PS_df)\n",
    "del PS_df\n",
    "\n",
    "# Standarizing column names in TP\n",
    "sperm.col_renamer(TP_df)\n",
    "df_l.append(TP_df)\n",
    "del TP_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we concatenate both DataFrame (`PS_df` and `TP_df`) into a single, harmonized DataFrame (`df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating our DataFrames\n",
    "df = pd.concat(df_l)\n",
    "\n",
    "# Listing the columns we want to keep after merging\n",
    "cols = ['File', 'Node', 'Scan', 'Int', 'RT (min)', 'm (Da)',\n",
    "        'Protein', 'Sequence', 'Ann. Seq.', 'Modifications',\n",
    "        'Start', 'Stop', 'Targets', 'Positions', 'Positions (abs.)',\n",
    "        'Probabilities', 'C-Score', '-log E', 'Proteoform ID', 'Count (ph)']\n",
    "\n",
    "# Slicing-in our columns of interest\n",
    "df = df[cols].copy()\n",
    "\n",
    "del cols, df_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Data wrangling\n",
    "\n",
    "Some columns were not present in `PS_df` or `TP_df` before concatenating and now appear as `np.nan`. We should amend this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling nans with empty strings\n",
    "string_nan_cols = ['Protein', 'Sequence', 'Modifications']\n",
    "df[string_nan_cols] = df[string_nan_cols].fillna(' ')\n",
    "\n",
    "# For each column in the column list...\n",
    "for col in ['Targets', 'Positions', 'Positions (abs.)', 'Probabilities']:\n",
    "\n",
    "    # ... mask-in the nans\n",
    "    nan_Î¼ = df[col].isna()\n",
    "\n",
    "    # ... fill those nans with empty lists using a lambda trick\n",
    "    df.loc[nan_Î¼, col] = df.loc[nan_Î¼, col].fillna('[]').apply(eval)\n",
    "\n",
    "del string_nan_cols, nan_Î¼, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the experimental design from the spectrum file names stored in the `'File'` column. Remember that we have two biological replicates (`'A'` and `'B'`) with two technical replicates each (`'R01'`, `'R02'` and `'R03'`, `'R04'`, respectively).<a id='Cell_c'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing file extension from the 'File' column\n",
    "df['File'] = df['File'].str.split('.').str[0]\n",
    "\n",
    "# Removing \"_ms2\" tag from 'File' column (only for TP)\n",
    "df['File'] = df['File'].replace({'_ms2': ''}, regex=True)\n",
    "\n",
    "# Extracting the experimental desing from the file name\n",
    "for k in sorted(e.keys(), reverse=True):\n",
    "\n",
    "    # Incorporating the experimental desing as categorical columns\n",
    "    df.insert(0, e[k], df['File'].str.split('_').str[k])\n",
    "\n",
    "del k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the Z-score of the logarithm in base 2 of the intensity (`'z log2 Int'`). Notice that the intensity in column `'Int'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the log2 of the intensity\n",
    "df.insert(loc=df.columns.get_loc('Int') + 1,\n",
    "          column='log2 Int',\n",
    "          value=np.log2(df['Int']))\n",
    "\n",
    "# Defining groupby columns (We want to compute z-score by Node and TechRep)\n",
    "gb_cols = ['Node', 'TechRep']\n",
    "\n",
    "# Getting the mean by Node and TechRep\n",
    "mean = df.groupby(by=gb_cols)['log2 Int'].transform(np.mean)\n",
    "\n",
    "# Getting the standard deviation by Node and TechRep\n",
    "std = df.groupby(by=gb_cols)['log2 Int'].transform(np.std)\n",
    "\n",
    "# Computting the z-score\n",
    "z = (df['log2 Int'] - mean) / std\n",
    "\n",
    "# Inserting the z-score of the intensity\n",
    "df.insert(loc=df.columns.get_loc('log2 Int') + 1,\n",
    "          column='z log2 Int',\n",
    "          value=z)\n",
    "\n",
    "del z, mean, std, gb_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will filter in those PrSMs with masses between 6100 Da and 18000 Da. Remember that the fully carbamidomethylated protamine forms lie in this mass range. Is important to filter the mass **BEFORE** the clustering in order to work with a fixed scaling factor for the mass dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking-in masses\n",
    "mass_Î¼ = (df['m (Da)'] > p['m_lower_lim']) & (df['m (Da)'] < p['m_upper_lim'])\n",
    "\n",
    "# Filtering-in masses\n",
    "df = df[mass_Î¼].copy()\n",
    "\n",
    "# Reporting in the console\n",
    "sum_ = sum(~mass_Î¼)\n",
    "len_ = len(mass_Î¼)\n",
    "print(f'm --> Dropped {sum_} of {len_} PrSMs ({sum_/len_ * 100:.2f}%)')\n",
    "\n",
    "del mass_Î¼, sum_, len_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving preclustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting values by 'm (Da)' and resetting index\n",
    "df = df.sort_values(by='m (Da)')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Trying to create the \"Output\" folder...\n",
    "try:\n",
    "    os.makedirs(f'{os.getcwd()}\\\\Output')\n",
    "\n",
    "# ... except if it is already created\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Exporting all dictionaries\n",
    "np.save(arr=p, file=f'{os.getcwd()}\\\\Output\\\\Project_dict.npy')\n",
    "np.save(arr=e, file=f'{os.getcwd()}\\\\Output\\\\Experiment_dict.npy')\n",
    "\n",
    "# Exporting the main DataFrame\n",
    "df.to_excel(f'{os.getcwd()}\\\\Output\\\\0_PrSMs_preDBSCAN.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - DBSCAN parameters optimization with Bokeh\n",
    "\n",
    "This part here requires your participation. We will leverage an interactive home-made web app written with the power Python package called \"Bokeh\" (you can check the source code within the `Proteomes_Bokeh.py` file). A new tab should open in your default web browsers after executing the next code cell. You should see the app dashboard a few seconds later. This app loads the `PrSMs_preDBSCAN.xlsx` file we just exported in the previous code cell and supports you in the task of optimizing $\\epsilon$ and $n_{min}$ DBSCAN parameters. By using this app, you can also filter by `'z log2 Int'` and `'RT (min)'` in order to get rid of low quality PrSMs and latest retention times coming from the column wash.\n",
    "\n",
    "Please, once you finish the optimization process, **shut down the Bokeh app by using the `Stop Bokeh Server` green button present in the dashboard**. If you don't stop the Bokeh app properly, the current kernel session that this Jupyter Notebook uses to compute things will remain busy. If this happens the following code cell will not run and you should restart the kernel by hand.\n",
    "\n",
    "Once you get your optimum parameters values, you can update the numbers stored within the `p` dictionary defined at the top of the code an rerun the Jupyter Notebook from the very beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the cmd from the same folder where Proteomes_Bokeh.py is located\n",
    "os.system('bokeh serve --show Proteomes_Bokeh.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our work, the values we get were: $\\epsilon = 1.2Â·10^{-3}$ and $n_{min} = 10$. (check the values stored in `p['Îµ']` and `p['n_min']` that we defined at the beginning of this Jupyter Notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing epsilon and n_min values> \n",
    "print(p['Îµ'])\n",
    "print(p['n_min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering values for the intensity we get were: $log_{2}(I_{z})_{min} = -10$ and $log_{2}(I_{z})_{max} = 10$. As you can see, we don't filtered out any PrSMs by their intensity, we just grab them all (check the values stored in `z-i_lower_lim` and `z-i_upper_lim` that we defined at the beginning of this Jupyter Notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing filtering window for the intensity (log2 of z-score of I) \n",
    "print(p['z-i_lower_lim'])\n",
    "print(p['z-i_upper_lim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering values for the retention time we get were: $RT_{min} = 0$ minutes and $RT_{max} = 42$ minutes. We filtered out those PrSMs coming from the latest retention times for being from the column wash (check the values stored in `rt_lower_lim` and `rt_upper_lim` that we defined at the beginning of this Jupyter Notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing filtering window for the retention time (log2 of z-score of I) \n",
    "print(p['rt_lower_lim'])\n",
    "print(p['rt_upper_lim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Filtering PrSMs by intensity and retention time\n",
    "\n",
    "Here we will apply the filtering windows we just get thanks to the Bokeh app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking-in intensity z-score\n",
    "zint_Î¼ = (df['z log2 Int'] > p['z-i_lower_lim']) & (df['z log2 Int'] < p['z-i_upper_lim'])\n",
    "\n",
    "# Filtering-in intensity z-score\n",
    "df = df[zint_Î¼].copy()\n",
    "\n",
    "# Reporting in the console\n",
    "sum_ = sum(~zint_Î¼)\n",
    "len_ = len(zint_Î¼)\n",
    "print(f'I --> Dropped {sum_} of {len_} PrSMs ({sum_/len_ * 100:.2f}%)')\n",
    "\n",
    "del zint_Î¼, sum_, len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking-in retention time\n",
    "rt_Î¼ = (df['RT (min)'] > p['rt_lower_lim']) & (df['RT (min)'] < p['rt_upper_lim'])\n",
    "\n",
    "# Filtering-in retention time\n",
    "df = df[rt_Î¼].copy()\n",
    "\n",
    "# Reporting in the console\n",
    "sum_ = sum(~rt_Î¼)\n",
    "len_ = len(rt_Î¼)\n",
    "print(f't --> Dropped {sum_} of {len_} PrSMs ({sum_/len_ * 100:.2f}%)')\n",
    "\n",
    "del rt_Î¼, sum_, len_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we will sent to the final DBSCAN algorithm 2838 PrSMs in total (56.6% from PS and 43.4% from TP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the total number of PrSMs (absolute) we will send to DBSCAN (by Node)\n",
    "print(df['Node'].value_counts())\n",
    "\n",
    "# Printing the total number of PrSMs (relative) we will send to DBSCAN (by Node)\n",
    "print(df['Node'].value_counts()/len(df['Node']) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - DBSCAN\n",
    "\n",
    "Now we will perform the DBSCAN clustering. This time we only clustered the `'m (Da)'`, but the idea is to cluster both `'m (Da)'` and `'RT (min)'` in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with DBSCAN after filtering\n",
    "sperm.DBSCAN_clustering(data=df, eps=p['Îµ'], min_samples=p['n_min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clustering, we successfully clustered 2578 PrSMs in total (55.9% from PS and 44.1% from TP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the total number of clustered PrSMs (absolute) (by Node)\n",
    "c_Î¼ = df['Cluster'] != 'Unclustered'\n",
    "print(df[c_Î¼]['Node'].value_counts())\n",
    "\n",
    "# Printing the total number of clustered PrSMs (relative) (by Node)\n",
    "print(df[c_Î¼]['Node'].value_counts() / len(df[c_Î¼]) * 100)\n",
    "\n",
    "del c_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that 260 PrSMs remained as unclustered (63.8% from PS and 36.2% from TP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the total number of unclustered PrSMs (absolute) (by Node)\n",
    "uc_Î¼ = df['Cluster'] == 'Unclustered'\n",
    "print(df[uc_Î¼]['Node'].value_counts())\n",
    "\n",
    "# Printing the total number of unclustered PrSMs (relative) (by Node)\n",
    "print(df[uc_Î¼]['Node'].value_counts() / len(df[uc_Î¼]) * 100)\n",
    "\n",
    "del uc_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - Cluster annotation (automatic phase)\n",
    "\n",
    "At this point we will perform a couple of protein searches (`'Protein (own search)'` and `'Proteins (own search)'`) to support the automatic cluster annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_search(seq, seq_d):\n",
    "    '''Protein (single) search (with full sequence match)'''\n",
    "    if seq in seq_d:\n",
    "        return seq_d.get(seq)\n",
    "    else:\n",
    "        return 'Unidentified'\n",
    "\n",
    "\n",
    "def proteins_search(seq, seq_d):\n",
    "    '''Proteins (multiple) search (with partial sequence match)'''\n",
    "    proteins = []\n",
    "    for protein, protein_name in seq_d.items():\n",
    "        if seq in protein:\n",
    "            proteins.append(protein_name)\n",
    "    return proteins\n",
    "\n",
    "\n",
    "# Searching if the sequence has a full match with a protein\n",
    "df.insert(loc=df.columns.get_loc('Protein') + 1,\n",
    "          column='Protein (own search)',\n",
    "          value=df['Sequence'].apply(protein_search, args=[seq_pf_dict]))\n",
    "\n",
    "# Searching if the sequence has a partial match with some proteins\n",
    "df.insert(loc=df.columns.get_loc('Protein') + 2,\n",
    "          column='Proteins (own search)',\n",
    "          value=df['Sequence'].apply(proteins_search, args=[seq_pf_dict]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the automatic cluster annotation process, we will prepare the skeleton of the `cluster_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by and aggregating 'Hits' with 'count'\n",
    "hits_df = df.groupby(by=['Cluster', 'Protein (own search)'], as_index=False).agg(Hits=('Scan', 'count'))\n",
    "\n",
    "# Grouping by and aggregating 'Hits' with max as 'Hits' and with sum as 'Size'\n",
    "cluster_df = hits_df.groupby(by=['Cluster']).agg(Hits=('Hits', 'max'),  Size=('Hits', 'sum'))\n",
    "\n",
    "# Getting cluster 'Hits' / 'Size' ratio\n",
    "cluster_df['Ratio'] = (cluster_df['Hits'] / cluster_df['Size']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatic cluster annotation process provisionally pre-assigns a protamine form to each cluster. To do that, we will assign to each cluster the protamine form with more hits within such cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Cluster' w/o aggregating and storing the indices with max 'Hits'\n",
    "index_max = hits_df.groupby(['Cluster'])['Hits'].idxmax()\n",
    "\n",
    "# Slicing-in the the \"champion\" proteins hits\n",
    "champions_df = hits_df.loc[index_max]\n",
    "\n",
    "# Setting the champion clusters as index of the DataFrame\n",
    "champions_df.index = champions_df['Cluster']\n",
    "\n",
    "# Storing the \"champion\" proteins hits to the clusters\n",
    "cluster_df['Cluster - Protein (Auto)'] = champions_df['Protein (own search)']\n",
    "\n",
    "del champions_df, index_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can annotate all the PrSMs in the `df` with this provisional protamine form pre-assignation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary linking a successful proteins with their clusters\n",
    "cluster_dict = dict(zip(cluster_df.index, cluster_df['Cluster - Protein (Auto)']))\n",
    "\n",
    "# Inserting the clusters to our data\n",
    "df.insert(loc=df.columns.get_loc('Cluster') + 1,\n",
    "          column='Cluster - Protein (Auto)',\n",
    "          value=df['Cluster'].map(cluster_dict))\n",
    "\n",
    "del cluster_dict, hits_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After annotating \"some\" PrSMs with a provisional protamine form, we can determine \"some\" provisional PrSMs mass shifts in our `df` DataFrame. Notice that we can only determine these provisional delta masses for those PrSMs having a provisional protamine form assigned. <a id='Cell_a'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Îm_computer(mass, protein, mass_dict):\n",
    "    '''Computes the mass shift of a PrSM given its mass and its cluster protein'''\n",
    "    if protein in mass_dict:\n",
    "        return mass - mass_dict[protein]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Computing the delta mass in the case of having a successful (auto) cluster\n",
    "Îm_ser = df.apply(lambda f: Îm_computer(f['m (Da)'],\n",
    "                                        f['Cluster - Protein (Auto)'],\n",
    "                                        pf_mass_dict), axis=1)\n",
    "\n",
    "# Adding the delta mass to our DataFrame\n",
    "df.insert(loc=df.columns.get_loc('m (Da)') + 1,\n",
    "          column='Îm (Da) (Auto)',\n",
    "          value=Îm_ser)\n",
    "\n",
    "del Îm_ser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can compute the mean mass and mean mass shift for each cluster (`'Cluster - m (Da)'` and ` 'Cluster - Îm (Da) (Auto)'`, respectively) to get the cluster annotation DataFrame (`cluster_df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the caracteristic 'm (Da)' and 'Îm (Da) (Auto)' for each cluster\n",
    "mass_df = df.groupby(['Cluster']).agg({'m (Da)': ['mean', 'std'], 'Îm (Da) (Auto)': ['mean']})\n",
    "# We added std aggregation of 'm (Da)' just as requested by Reviewer #1\n",
    "\n",
    "# Flattening column multiindex for simplicity\n",
    "mass_df.columns = ['|'.join(col) for col in mass_df.columns.values]\n",
    "\n",
    "# Defining renaming dictionary for the mass_df (aesthetics)\n",
    "rename_dict = {\n",
    "               'm (Da)|mean': 'Cluster - m (Da) (mean)',\n",
    "               'm (Da)|std': 'Cluster - m (Da) (std)',\n",
    "               'Îm (Da) (Auto)|mean': 'Cluster - Îm (Da) (Auto)',\n",
    "               }\n",
    "\n",
    "# Renaming Cluster DataFrame columns\n",
    "mass_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "del rename_dict\n",
    "\n",
    "# Incorporating mean 'm (Da)' and 'Îm (Da) (Auto)' to cluster_df\n",
    "cluster_df = pd.concat([cluster_df, mass_df], axis=1)\n",
    "\n",
    "# Inserting the \"unknown\" 'Protein (Revised)' column\n",
    "cluster_df.insert(loc=cluster_df.columns.get_loc('Cluster - Protein (Auto)') + 1,\n",
    "                  column='Cluster - Protein (Revised)',\n",
    "                  value='Unknown')\n",
    "\n",
    "# Inserting \"unknown\" 'Proteoform (Revised)'and Modification (Revised)' columns\n",
    "cluster_df['Cluster - Proteoform (Revised)'] = 'Unknown'\n",
    "cluster_df['Cluster - Modification (Revised)'] = 'Unknown'\n",
    "\n",
    "del mass_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are able to export the PrSM DataFrame (`df`) and the cluster annotation DataFrame (`cluster_df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the main DataFrame\n",
    "df.to_excel(f'{os.getcwd()}\\\\Output\\\\1_PrSMs_postDBSCAN.xlsx', index=False)\n",
    "\n",
    "# Trying to create the \"Annotation\" folder...\n",
    "try:\n",
    "    os.makedirs(f'{os.getcwd()}\\\\Output\\\\Annotation')\n",
    "\n",
    "# ... only if it is already created\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Exporting the cluster hits DataFrame and cluster automatic annotation\n",
    "cluster_df.to_excel(f'{os.getcwd()}\\\\Output\\\\Annotation\\\\Cluster.xlsx')\n",
    "\n",
    "del cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8 - Cluster annotation (manual phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment the automatic cluster annotation requires manual validation. To this aim we duplicated the `Cluster.xlsx` spreadsheet we just exported by hand as a new `Cluster (Revised).xlsx` spreadsheet. Then we open and manually annotate the columns `'Cluster - Protein (Revised)'`, `'Cluster - Proteoform (Revised)'` and `'Cluster - Modification (Revised)'` whenever is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Cluster human annotation\n",
    "cluster_rev_df = pd.read_excel(f'{os.getcwd()}\\\\Output\\\\Annotation\\\\Cluster (Revised).xlsx', index_col='Cluster', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manual validation, we should update some mass shift values within `cluster_rev_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Îm(mass, protein, mass_dict):\n",
    "    '''Computes the mass shift of a PrSM given its mass and its cluster protein'''\n",
    "    if protein in mass_dict:\n",
    "        return mass - mass_dict[protein]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Updating the Îm after the human annotation\n",
    "Îm_ser = cluster_rev_df.apply(lambda f: Îm(f['Cluster - m (Da) (mean)'],\n",
    "                                           f['Cluster - Protein (Revised)'],\n",
    "                                           pf_mass_dict), axis=1)\n",
    "\n",
    "# Adding the updated Îm to our Cluster DataFrame\n",
    "cluster_rev_df.insert(loc=cluster_rev_df.columns.get_loc('Cluster - Îm (Da) (Auto)') + 1,\n",
    "                      column='Cluster - Îm (Da) (Revised)',\n",
    "                      value=Îm_ser)\n",
    "\n",
    "del Îm_ser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now we have a nice and well annotated `cluster_rev_df` DataFrame, we will use it to annotate all the PrSMs within our `df` DataFrame by linking their corresponding clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the cluster list for incomming (multiple) annotations\n",
    "clusters = cluster_rev_df.index\n",
    "\n",
    "# Annotating 'Cluster - Îm (Da) (Revised)'\n",
    "map_dict = dict(zip(clusters, cluster_rev_df['Cluster - Îm (Da) (Revised)']))\n",
    "df.insert(loc=df.columns.get_loc('Cluster') + 1,\n",
    "          column='Cluster - Îm (Da) (Revised)',\n",
    "          value=df['Cluster'].map(map_dict))\n",
    "\n",
    "# Annotating 'Cluster - m (Da) (mean)'\n",
    "map_dict = dict(zip(clusters, cluster_rev_df['Cluster - m (Da) (mean)']))\n",
    "df.insert(loc=df.columns.get_loc('Cluster') + 1,\n",
    "          column='Cluster - m (Da) (mean)',\n",
    "          value=df['Cluster'].map(map_dict))\n",
    "\n",
    "# Annotating 'Cluster - m (Da) (std)'\n",
    "map_dict = dict(zip(clusters, cluster_rev_df['Cluster - m (Da) (std)']))\n",
    "df.insert(loc=df.columns.get_loc('Cluster') + 1,\n",
    "          column='Cluster - m (Da) (std)',\n",
    "          value=df['Cluster'].map(map_dict))\n",
    "\n",
    "# Annotating 'Cluster - Modification (Revised)'\n",
    "map_dict = dict(zip(clusters, cluster_rev_df['Cluster - Modification (Revised)']))\n",
    "df.insert(loc=df.columns.get_loc('Cluster - Îm (Da) (Revised)') + 1,\n",
    "          column='Cluster - Modification (Revised)',\n",
    "          value=df['Cluster'].map(map_dict))\n",
    "\n",
    "# Annotating 'Cluster - Proteoform (Revised)'\n",
    "map_dict = dict(zip(clusters, cluster_rev_df['Cluster - Proteoform (Revised)']))\n",
    "df.insert(loc=df.columns.get_loc('Cluster - Îm (Da) (Revised)') + 1,\n",
    "          column='Cluster - Proteoform (Revised)',\n",
    "          value=df['Cluster'].map(map_dict))\n",
    "\n",
    "# Annotating 'Cluster - Protein (Revised)'\n",
    "map_dict = dict(zip(clusters, cluster_rev_df['Cluster - Protein (Revised)']))\n",
    "df.insert(loc=df.columns.get_loc('Cluster - Îm (Da) (Revised)') + 1,\n",
    "          column='Cluster - Protein (Revised)',\n",
    "          value=df['Cluster'].map(map_dict))\n",
    "\n",
    "# Creating 'Cluster â Proteoform' column to use when plotting figures (easthetics)\n",
    "df['Cluster â Proteoform'] = df['Cluster'] + ' â ' + df['Cluster - Proteoform (Revised)']\n",
    "\n",
    "del clusters, map_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly as we did in this previous [code cell](#Cell_a), we can now recompute the mass shift but now referred to the `'Cluster - Protein (Revised)'` instead of the `'Cluster - Protein (Auto)'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the delta mass in the case of having a successful (revised) cluster\n",
    "Îm_ser = df.apply(lambda f: Îm(f['m (Da)'],\n",
    "                               f['Cluster - Protein (Revised)'],\n",
    "                               pf_mass_dict), axis=1)\n",
    "\n",
    "# Adding the delta mass to our DataFrame\n",
    "df.insert(loc=df.columns.get_loc('Îm (Da) (Auto)') + 1,\n",
    "          column='Îm (Da) (Revised)',\n",
    "          value=Îm_ser)\n",
    "\n",
    "del Îm_ser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we should export the resulting cluster annotation DataFrame (`cluster_rev_df`) and PrSMs DataFrame (`df`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the updated Cluster DataFrame\n",
    "cluster_rev_df.to_excel(f'{os.getcwd()}\\\\Output\\\\2_Protamine_Ann (Table S1).xlsx', index=True)\n",
    "\n",
    "# Exporting the main DataFrame with proteoform annotation after clustering\n",
    "df.to_excel(f'{os.getcwd()}\\\\Output\\\\3_PrSMs_postDBSCAN_Ann (Table S2).xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: These two files are mentioned as a part of the supplementary material in the paper body, namely as **Table S1** and **Table S2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9 - Reproducing Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before generating **Table 1**, we need to round some magnitudes to avoid being to cumbersome. Notice that columns `'m_T (Da)'`,  `'m_C (Da)'` and  `'Îm_CT (Da)'` correspond to the three magnitudes appearing in the Equation 2 from the paper as  $m_T$, $m_C$ and $\\Delta m_{CT}$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some columns with rounded values and new names (just to get nicer header names in upcomming pivot table)\n",
    "pf_mass_round_dict = {k: round(pf_mass_dict[k], 2) for k in pf_mass_dict} \n",
    "df['m_T (Da)'] = df['Cluster - Proteoform (Revised)'].replace(pf_mass_round_dict)\n",
    "df['m_C (Da) (mean)'] = df['Cluster - m (Da) (mean)'].round(2)\n",
    "df['m_C (Da) (std)'] = df['Cluster - m (Da) (std)'].round(2)\n",
    "df['Îm_CT (Da)'] = df['Cluster - Îm (Da) (Revised)'].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will reproduce **Table 1** appearing in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicating some columns with new names (just to get nicer header names in upcomming pivot table)\n",
    "df['Proteoform'] = df['Cluster - Proteoform (Revised)'].copy()\n",
    "df['PF count'] = df['Proteoform ID'].copy()\n",
    "df['PrSM count'] = df['Scan'].copy()\n",
    "\n",
    "# Filling nan values in 'Îm_CT' column (just to not lose associated rows in upcomming pivot table)\n",
    "df['Îm_CT (Da)'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Pivoting and aggregating:\n",
    "#'PF count' with number of distinct observations ('nunique')\n",
    "#'PF count' with number observations ('count')\n",
    "tab2 = pd.pivot_table(\n",
    "                      df,\n",
    "                      values=['PF count', 'PrSM count'],\n",
    "                      index=['Cluster', 'Proteoform', 'm_T (Da)', 'm_C (Da) (mean)', 'm_C (Da) (std)', 'Îm_CT (Da)'],\n",
    "                      columns=['Node'],\n",
    "                      aggfunc={'PF count': 'nunique', 'PrSM count': 'count'},\n",
    "                      )\n",
    "# Showing tab2\n",
    "tab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Subsection 3.2.1 from the paper we give `'mean'` and `'max'` number of proteoforms per cluster according to PS and TP. Let's reproduce these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the Unclustered row from tab2\n",
    "tab2.drop(labels=['Unclustered'], axis=0, inplace=True)\n",
    "\n",
    "# Computing the mean number of proteoforms per cluster for PS and TP\n",
    "print(tab2['PF count'].apply('mean'))\n",
    "\n",
    "# Computing the top number of proteoforms per cluster for PS and TP\n",
    "print(tab2['PF count'].apply('max'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns\n",
    "df.drop(columns=['Proteoform', 'PF count', 'PrSM count', 'm_T (Da)', 'Îm_CT (Da)', 'm_C (Da) (mean)', 'm_C (Da) (std)'], inplace=True)\n",
    "\n",
    "del pf_mass_round_dict, tab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10 - Reproducing Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will reproduce **Figure 3a** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to create the \"Figures\" folder...\n",
    "try:\n",
    "    os.makedirs(f'{os.getcwd()}\\\\Output\\\\Figures\\\\png')\n",
    "    os.makedirs(f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf')\n",
    "    os.makedirs(f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\png')\n",
    "    os.makedirs(f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\pdf')\n",
    "\n",
    "# ... only if it is already created\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a palette with a different color for each cluster\n",
    "pal = sns.color_palette('hls', len(df['Cluster â Proteoform'].unique()))\n",
    "\n",
    "# HEX-converting the palette to make it suitable for ggplot\n",
    "pal = list(pal.as_hex())\n",
    "\n",
    "# Defining the last color of the palette as grey (for unclustered PrSMs)\n",
    "pal[-1] = '#808080'\n",
    "\n",
    "# Defining the figure size 1.2 x (7, 4)\n",
    "p9.options.figure_size = (8.4, 4.8)\n",
    "\n",
    "# Generating our ggplot\n",
    "Fig3a = (ggplot(\n",
    "                df[(df['m (Da)'] > 7000) & (df['m (Da)'] < 14000)].copy(),\n",
    "                aes(\n",
    "                    x='m (Da)',\n",
    "                    y='RT (min)',\n",
    "                    color='Cluster â Proteoform',\n",
    "                    fill='Cluster â Proteoform',\n",
    "                    )\n",
    "                )\n",
    "         + geom_point(\n",
    "                      alpha=0.25,\n",
    "                      )\n",
    "         + geom_rect(\n",
    "                     mapping=aes(xmin=8950, xmax=9400, ymin=0, ymax=42),\n",
    "                     linetype='dotted',\n",
    "                     color='#808080',\n",
    "                     fill='#808080',\n",
    "                     alpha=0,\n",
    "                     size=0.25\n",
    "                     )\n",
    "         + facet_wrap(facets=['Node'], ncol=1)\n",
    "         + scale_fill_manual(values=pal)\n",
    "         + scale_color_manual(values=pal)\n",
    "         + guides(colour=guide_legend(override_aes={'alpha': 1}))\n",
    "         + theme_bw()\n",
    "         + theme(\n",
    "                 legend_position=\"top\",\n",
    "                 legend_key_size=7,\n",
    "                 legend_title=element_text(size=8),\n",
    "                 legend_text=element_text(size=6)\n",
    "                 )\n",
    "         )\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig3a.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig3a.png', verbose=False)\n",
    "Fig3a.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig3a.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will reproduce **Figure 3b** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (4, 4)\n",
    "p9.options.figure_size = (4.8, 4.8)\n",
    "\n",
    "# Filtering-in the mass windown we want to show in Fig3b and Fig3c\n",
    "df_fig3bc = df[(df['m (Da)'] > 8950) & (df['m (Da)'] < 9400)].copy()\n",
    "\n",
    "# Generating our ggplot\n",
    "Fig3b = (ggplot(\n",
    "                df_fig3bc,\n",
    "                aes(\n",
    "                    x='m (Da)',\n",
    "                    y='RT (min)',\n",
    "                    color='Proteoform ID',\n",
    "                    fill='Proteoform ID',\n",
    "                    )\n",
    "                )\n",
    "          + geom_point(\n",
    "                       alpha=0.75,\n",
    "                       )\n",
    "          + facet_wrap(facets=['Node'], ncol=1)\n",
    "          + guides(colour=guide_legend(ncol=7, override_aes={'alpha': 1}))\n",
    "          + theme_bw()\n",
    "          + xlim(8950, 9400)\n",
    "          + theme(\n",
    "                  legend_position=\"top\",\n",
    "                  legend_key_size=7,\n",
    "                  legend_title=element_text(size=8),\n",
    "                  legend_text=element_text(size=6)\n",
    "                  )\n",
    "          )\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig3b.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig3b.png', verbose=False)\n",
    "Fig3b.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig3b.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will reproduce **Figure 3c** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a palette with a different color for each cluster\n",
    "pal = sns.color_palette('hls', len(df_fig3bc['Cluster â Proteoform'].unique()))\n",
    "\n",
    "# HEX-converting the palette to make it suitable for ggplot\n",
    "pal = list(pal.as_hex())\n",
    "\n",
    "# Defining the last color of the palette as grey (for unclustered PrSMs)\n",
    "pal[-1] = '#808080'\n",
    "\n",
    "# Defining the figure size 1.2 x (4, 4)\n",
    "p9.options.figure_size = (4.8, 4.8)\n",
    "\n",
    "# Generating our ggplot\n",
    "Fig3c = (ggplot(\n",
    "                df_fig3bc,\n",
    "                aes(\n",
    "                    x='m (Da)',\n",
    "                    y='RT (min)',\n",
    "                    color='Cluster â Proteoform',\n",
    "                    fill='Cluster â Proteoform',\n",
    "                    )\n",
    "                )\n",
    "          + geom_point(\n",
    "                       alpha=0.75,\n",
    "                       )\n",
    "          + facet_wrap(facets=['Node'], ncol=1)\n",
    "          + scale_fill_manual(values=pal)\n",
    "          + scale_color_manual(values=pal)\n",
    "          + guides(colour=guide_legend(nrow=1, override_aes={'alpha': 1}))\n",
    "          + theme_bw()\n",
    "          + xlim(8950, 9400)\n",
    "          + theme(\n",
    "                  legend_position=\"top\",\n",
    "                  legend_key_size=7,\n",
    "                  legend_title=element_text(size=8),\n",
    "                  legend_text=element_text(size=6)\n",
    "                  )\n",
    "          )\n",
    "\n",
    "del pal\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig3c.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig3c.png', verbose=False)\n",
    "Fig3c.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig3c.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Fig3a, Fig3b, Fig3c, df_fig3bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11 - Reproducing Supplementary Figure 1\n",
    "\n",
    "Here we will reproduce **Figure S1a** just as appears in the supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a palette with a different color for each cluster\n",
    "pal = sns.color_palette('hls', len(df['Cluster â Proteoform'].unique()))\n",
    "\n",
    "# HEX-converting the palette to make it suitable for ggplot\n",
    "pal = list(pal.as_hex())\n",
    "\n",
    "# Defining the last color of the palette as grey (for unclustered PrSMs)\n",
    "pal[-1] = '#808080'\n",
    "\n",
    "# Defining the figure size 1.2 x (7, 4)\n",
    "p9.options.figure_size = (8.4, 4.8)\n",
    "\n",
    "# Generating our ggplot\n",
    "FigS1a = (ggplot(\n",
    "                 df,\n",
    "                 aes(\n",
    "                     x='m (Da)',\n",
    "                     y='log2 Int',\n",
    "                     color='Cluster â Proteoform',\n",
    "                     fill='Cluster â Proteoform',\n",
    "                     )\n",
    "                 )\n",
    "          + geom_point(\n",
    "                       alpha=0.25,\n",
    "                       )\n",
    "          + facet_wrap(facets=['Node'], ncol=1)\n",
    "          + scale_fill_manual(values=pal)\n",
    "          + scale_color_manual(values=pal)\n",
    "          + guides(colour=guide_legend(override_aes={'alpha': 1}))\n",
    "          + theme_bw()\n",
    "          + theme(\n",
    "                  legend_position=\"top\",\n",
    "                  legend_key_size=7,\n",
    "                  legend_title=element_text(size=8),\n",
    "                  legend_text=element_text(size=6)\n",
    "                  )\n",
    "          )\n",
    "\n",
    "# Saving our ggplot\n",
    "FigS1a.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\png\\\\FigS1a.png', verbose=False)\n",
    "FigS1a.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\pdf\\\\FigS1a.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "FigS1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we will reproduce **Figure S1b** just as appears in the supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a palette with a different color for each cluster\n",
    "pal = sns.color_palette('hls', len(df['Cluster â Proteoform'].unique()))\n",
    "\n",
    "# HEX-converting the palette to make it suitable for ggplot\n",
    "pal = list(pal.as_hex())\n",
    "\n",
    "# Defining the last color of the palette as grey (for unclustered PrSMs)\n",
    "pal[-1] = '#808080'\n",
    "\n",
    "# Defining the figure size 1.2 x (7, 4)\n",
    "p9.options.figure_size = (8.4, 4.8)\n",
    "\n",
    "# Generating our ggplot\n",
    "FigS1b = (ggplot(\n",
    "                df,\n",
    "                aes(\n",
    "                    x='RT (min)',\n",
    "                    y='log2 Int',\n",
    "                    color='Cluster â Proteoform',\n",
    "                    fill='Cluster â Proteoform',\n",
    "                    )\n",
    "                )\n",
    "         + geom_point(\n",
    "                      alpha=0.25,\n",
    "                      )\n",
    "         + facet_wrap(facets=['Node'], ncol=1)\n",
    "         + scale_fill_manual(values=pal)\n",
    "         + scale_color_manual(values=pal)\n",
    "         + guides(colour=guide_legend(override_aes={'alpha': 1}))\n",
    "         + theme_bw()\n",
    "         + theme(\n",
    "                 legend_position=\"top\",\n",
    "                 legend_key_size=7,\n",
    "                 legend_title=element_text(size=8),\n",
    "                 legend_text=element_text(size=6)\n",
    "                 )\n",
    "         )\n",
    "\n",
    "# Saving our ggplot\n",
    "FigS1b.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\png\\\\FigS1b.png', verbose=False)\n",
    "FigS1b.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\pdf\\\\FigS1b.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "FigS1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del FigS1a, FigS1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12 - Reproducing Supplementary Figure 2\n",
    "\n",
    "Here we will reproduce **Figure S2a** just as appears in the supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (7, 4)\n",
    "p9.options.figure_size = (8.4, 4.8)\n",
    "\n",
    "# Generating our ggplot\n",
    "FigS2a = (ggplot(\n",
    "                 df[df['Node'] == 'PS'],\n",
    "                 aes(\n",
    "                     x='m (Da)',\n",
    "                     y='RT (min)',\n",
    "                     color='Proteoform ID',\n",
    "                     fill='Proteoform ID',\n",
    "                     )\n",
    "                 )\n",
    "            + geom_point(\n",
    "                         alpha=0.25,\n",
    "                         )\n",
    "            + facet_wrap(facets=['Node'], ncol=1)\n",
    "            + guides(colour=guide_legend(override_aes={'alpha': 1}))\n",
    "            + theme_bw()\n",
    "            + theme(\n",
    "                    legend_position=\"right\",\n",
    "                    legend_key_size=7,\n",
    "                    legend_title=element_text(size=8),\n",
    "                    legend_text=element_text(size=6)\n",
    "                    )\n",
    "            )\n",
    "\n",
    "# Saving our ggplot\n",
    "FigS2a.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\png\\\\FigS2a.png', verbose=False)\n",
    "FigS2a.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\pdf\\\\FigS2a.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "FigS2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we will reproduce **Figure S2b** just as appears in the supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (7, 4)\n",
    "p9.options.figure_size = (8.4, 4.8)\n",
    "\n",
    "# Generating our ggplot\n",
    "FigS2b = (ggplot(\n",
    "                 df[df['Node'] == 'TP'],\n",
    "                 aes(\n",
    "                     x='m (Da)',\n",
    "                     y='RT (min)',\n",
    "                     color='Proteoform ID',\n",
    "                     fill='Proteoform ID',\n",
    "                     )\n",
    "                 )\n",
    "          + geom_point(\n",
    "                       alpha=0.25,\n",
    "                       )\n",
    "          + facet_wrap(facets=['Node'], ncol=1)\n",
    "          + guides(colour=guide_legend(override_aes={'alpha': 1}))\n",
    "          + theme_bw()\n",
    "          + theme(\n",
    "                  legend_position=\"right\",\n",
    "                  legend_key_size=7,\n",
    "                  legend_title=element_text(size=8),\n",
    "                  legend_text=element_text(size=6)\n",
    "                  )\n",
    "          )\n",
    "\n",
    "# Saving our ggplot\n",
    "FigS2b.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\png\\\\FigS2b.png', verbose=False)\n",
    "FigS2b.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\pdf\\\\FigS2b.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "FigS2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del FigS2a, FigS2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13 - Reproducing Supplementary Figure 3\n",
    "\n",
    "Before reproducing **Figure S3a** and **Figure S3b**, we need to differentiate between classified and unclassified PrSMs. <a id='Cell_b'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking classified/unclassified PrSMs (those that are known/unknown)\n",
    "c_Î¼ = df['Cluster - Protein (Revised)'] != 'Unknown'\n",
    "uc_Î¼ = df['Cluster - Protein (Revised)'] == 'Unknown'\n",
    "\n",
    "# Tagging classified/unclassified PrSMs for incomming FigS6a and FigS6b\n",
    "df.loc[c_Î¼, 'Outcome'] = 'Classified'\n",
    "df.loc[uc_Î¼, 'Outcome'] = 'Unclassified'\n",
    "\n",
    "del uc_Î¼, c_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's compute some summary statistics numbers that we state in the paper body (Subsection 3.2.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by 'Outcome' and counting thr number of PrSMs\n",
    "pvt = df.groupby(by=['Outcome']).agg(Count=('Node', 'count'))\n",
    "\n",
    "# Computing the PrSM count ratio by 'Outcome'\n",
    "pvt['Ratio (%)'] = (pvt['Count'] / len(df)) * 100\n",
    "\n",
    "# Showing table\n",
    "pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping '-log E', 'z log2 Int' by 'Outcome' and 'Node' and aggregating both with 'mean'\n",
    "df.groupby(by=['Outcome', 'Node']).agg({'-log E': 'mean', 'z log2 Int': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will reproduce **Figure S3a** just as appears in the supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a palette with a different color for each outcome\n",
    "pal = sns.color_palette('hls', len(df['Outcome'].unique()))\n",
    "\n",
    "# HEX-converting the palette to make it suitable for ggplot\n",
    "pal = list(pal.as_hex())\n",
    "\n",
    "# Defining the figure size 1.2 x (4, 4)\n",
    "p9.options.figure_size = (4.8, 4.8)\n",
    "\n",
    "# Generating our ggplot\n",
    "FigS3a = (ggplot(\n",
    "                 df,\n",
    "                 aes(\n",
    "                     x='Node',\n",
    "                     y='-log E',\n",
    "                     color='Outcome',\n",
    "                     fill='Outcome',\n",
    "                     )\n",
    "                 )\n",
    "          + geom_boxplot(\n",
    "                          alpha=0.75,\n",
    "                          outlier_alpha=0.75,\n",
    "                          )\n",
    "          + scale_fill_manual(values=pal[::-1])\n",
    "          + scale_color_manual(values=pal[::-1])\n",
    "          + guides(colour=guide_legend(override_aes={'alpha': 1}))\n",
    "          + theme_bw()\n",
    "          + xlab('Node')\n",
    "          + ylab('-logââ E-value')\n",
    "          )\n",
    "\n",
    "del pal, pvt\n",
    "\n",
    "# Saving our ggplot\n",
    "FigS3a.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\png\\\\FigS3a.png', verbose=False)\n",
    "FigS3a.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\pdf\\\\FigS3a.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "FigS3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we will reproduce **Figure S3b** just as appears in the supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a palette with a different color for each outcome\n",
    "pal = sns.color_palette('hls', len(df['Outcome'].unique()))\n",
    "\n",
    "# HEX-converting the palette to make it suitable for ggplot\n",
    "pal = list(pal.as_hex())\n",
    "\n",
    "# Defining the figure size 1.2 x (4, 4)\n",
    "p9.options.figure_size = (4.8, 4.8)\n",
    "\n",
    "# Generating our ggplot\n",
    "FigS3b = (ggplot(\n",
    "                 df,\n",
    "                 aes(\n",
    "                     x='Node',\n",
    "                     y='z log2 Int',\n",
    "                     color='Outcome',\n",
    "                     fill='Outcome',\n",
    "                     )\n",
    "                 )\n",
    "          + geom_boxplot(\n",
    "                         alpha=0.75,\n",
    "                         outlier_alpha=0.75,\n",
    "                         )\n",
    "          + scale_fill_manual(values=pal[::-1])\n",
    "          + scale_color_manual(values=pal[::-1])\n",
    "          + guides(colour=guide_legend(override_aes={'alpha': 1}))\n",
    "          + theme_bw()\n",
    "          + xlab('Node')\n",
    "          + ylab('z-score of logâ intensity')\n",
    "          )\n",
    "\n",
    "del pal\n",
    "\n",
    "# Saving our ggplot\n",
    "FigS3b.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\png\\\\FigS3b.png', verbose=False)\n",
    "FigS3b.save(filename=f'{os.getcwd()}\\\\Output\\\\SupplementaryFigures\\\\pdf\\\\FigS3b.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "FigS3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del FigS3a, FigS3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 14 - LandScape plots preparation\n",
    "\n",
    "In order to prepare protamine proteoform LansScape plots, we just new a few columns from our PrSMs DataFrame `df`. We are going to slice-in those columns to work with an simplified DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definig list of columns to slice-in\n",
    "col1_list = [\n",
    "             'Cluster - Protein (Revised)',\n",
    "             'Cluster - Proteoform (Revised)',\n",
    "             'Cluster - Îm (Da) (Revised)',\n",
    "             'Cluster - Modification (Revised)',\n",
    "             'Cluster - m (Da) (mean)',\n",
    "             'log2 Int',\n",
    "             'z log2 Int',\n",
    "             'Outcome',\n",
    "             ]\n",
    "\n",
    "# Definig list of experimental desing columns to slice-in\n",
    "col2_list = list(e.values()) + ['Node']\n",
    "\n",
    "# Slicing-in interesting columns\n",
    "df_landscape = df[col2_list + col1_list].copy()\n",
    "\n",
    "del col2_list, col1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names we used in `df` are a bit bulky. Let's simplify them a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining standardizing renaming dictionary\n",
    "uniform_dict = {\n",
    "                'Cluster - Protein (Revised)':      'Form',\n",
    "                'Cluster - Proteoform (Revised)':   'Proteoform',\n",
    "                'Cluster - Îm (Da) (Revised)':      'Îm (Da)',\n",
    "                'Cluster - Modification (Revised)': 'Modification',\n",
    "                'Cluster - m (Da) (mean)':          'm (Da)',\n",
    "                'log2 Int':                         'Intensity (log2)',\n",
    "                'z log2 Int':                       'Intensity (z)',\n",
    "                }\n",
    "\n",
    "# Standarizing column names\n",
    "df_landscape.rename(columns=uniform_dict, inplace=True)\n",
    "\n",
    "del uniform_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will drop *unclassified* PrSMs (you can revisit this [code cell](#Cell_b) to recall this *classified* / *unclassified* categorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering in classified PrSMs for incomming LandScape plots\n",
    "c_Î¼ = df_landscape['Outcome'] == 'Classified'\n",
    "df_landscape_f = df_landscape[c_Î¼].copy()\n",
    "\n",
    "del c_Î¼, df_landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code chunk is the building block to generate a LandScape plot. We are going to prepare LandScapes at three levels (Global, BioRep and TechRep). In this first code cell we will prepare the LandScape data at the Global level. After, the execution, the data to generate the LandScape plot at the Global level will be stored in a DataFrame within the `df_gb_dict` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating dataframe dictionary for incomming groupby\n",
    "df_gb_dict = {}\n",
    "\n",
    "# Rounding 'Îm (Da)' to enable proper grouping by\n",
    "df_landscape_f['Îm (Da)'] = df_landscape_f['Îm (Da)'].round(2)\n",
    "\n",
    "# Defining a list with the common grouping categories for incomming groupby\n",
    "g_list = ['Form', 'Proteoform', 'Modification', 'Îm (Da)']\n",
    "# We let 'Node' out to combine all the hits for both softs\n",
    "\n",
    "# Initiating Excel writer to save spreadsheet with multiple sheets\n",
    "writer = pd.ExcelWriter(f'{os.getcwd()}\\\\Output\\\\4_LandScapes.xlsx')\n",
    "\n",
    "# Grouping by and aggregating 'I' with size to get number of hits...\n",
    "gb_list = g_list\n",
    "df_gb = df_landscape_f.groupby(by=gb_list).agg(PrSMs=('Outcome', 'count'))\n",
    "df_gb.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# Sorting the main DataFrame to get sorted list of factors (hue)\n",
    "df_gb.sort_values(by='Îm (Da)', inplace=True, ascending=False)\n",
    "\n",
    "# Stablishing 'Modification' order to get a sorted hue\n",
    "sorter_mod = list(df_gb['Modification'].unique())\n",
    "df_gb['Modification'] = df_gb['Modification'].astype('category')\n",
    "df_gb['Modification'].cat.set_categories(sorter_mod, inplace=True)\n",
    "\n",
    "# Stablishing (by hand) 'Form' order to get a sorted x axis\n",
    "sorter_prot = ['P1', 'HP3', 'HP2', 'HP4', 'HPS1', 'HPS2', 'HPI2', 'pre-P2']\n",
    "df_gb['Form'] = df_gb['Form'].astype('category')\n",
    "df_gb['Form'].cat.set_categories(sorter_prot, inplace=True)\n",
    "\n",
    "# Storing the grouped DataFrame grouped in the DataFrame dictionary\n",
    "df_gb_dict['Global'] = df_gb\n",
    "\n",
    "# Writing the grouped DataFrame grouped\n",
    "df_gb.to_excel(writer, sheet_name='Global', index=False)\n",
    "\n",
    "del gb_list, df_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly as we just did, now we are going to prepare the LandScape data at the BioRep and TechRep levels. We will implement a for loop to this aim. After the execution, the `df_gb_dict` dictionary should contain the LandScape data at all levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each category in the experimental design dictionary...\n",
    "for cat in e.values():\n",
    "\n",
    "    # Grouping by and aggregating 'I' with size to get number of hits...\n",
    "    gb_list = [cat] + g_list\n",
    "    df_gb = df_landscape_f.groupby(by=gb_list).agg(PrSMs=('Outcome', 'count'))\n",
    "    df_gb.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    # Sorting the main DataFrame to get sorted list of factors (hue)\n",
    "    df_gb.sort_values(by='Îm (Da)', inplace=True, ascending=False)\n",
    "\n",
    "    # Stablishing 'Modification' order to get a sorted hue\n",
    "    sorter_mod = list(df_gb['Modification'].unique())\n",
    "    df_gb['Modification'] = df_gb['Modification'].astype('category')\n",
    "    df_gb['Modification'].cat.set_categories(sorter_mod, inplace=True)\n",
    "\n",
    "    # Stablishing (by hand) 'Form' order to get a a sorted x axis\n",
    "    sorter_prot = ['P1', 'HP3', 'HP2', 'HP4', 'HPS1', 'HPS2', 'HPI2', 'pre-P2']\n",
    "    df_gb['Form'] = df_gb['Form'].astype('category')\n",
    "    df_gb['Form'].cat.set_categories(sorter_prot, inplace=True)\n",
    "\n",
    "    # Storing the grouped DataFrame grouped in the DataFrame dictionary\n",
    "    df_gb_dict[cat] = df_gb\n",
    "\n",
    "    # Writing the grouped DataFrame grouped\n",
    "    df_gb.to_excel(writer, sheet_name=cat, index=False)\n",
    "\n",
    "del cat, g_list, df_gb, gb_list, sorter_mod, sorter_prot, df_landscape_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will export the LandScape data as an spreadsheet with a separated sheet for each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our Excel spreadsheet with LandScape data using multiple sheets \n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 15 - Reproducing Figure 4 (LandScapes)\n",
    "\n",
    "Here we will reproduce **Figure 4a** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (5, 3)\n",
    "p9.options.figure_size = (6.0, 3.6)\n",
    "\n",
    "# Extracting LandScapes data by 'Global'\n",
    "df_gb = df_gb_dict['Global']\n",
    "\n",
    "# Plotting with ggplot\n",
    "Fig4a = (ggplot(df_gb, aes(\n",
    "                           x='Form',\n",
    "                           y='Îm (Da)',\n",
    "                           color='Modification',\n",
    "                           size='PrSMs',\n",
    "                           )\n",
    "                )\n",
    "         + ggtitle(f'Proteoform landscape (Global)')\n",
    "         + geom_point(alpha=1)\n",
    "         + theme_bw()\n",
    "         + theme(axis_text_x=element_text(angle=0))\n",
    "         + theme(\n",
    "                 legend_position=\"right\",\n",
    "                 legend_key_size=7,\n",
    "                 legend_title = element_text(size=8),\n",
    "                 legend_text = element_text(size=6),\n",
    "                 )\n",
    "         )\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig4a.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig4a.png', verbose=False)\n",
    "Fig4a.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig4a.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will reproduce **Figure 4b** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (5, 3)\n",
    "p9.options.figure_size = (6.0, 3.6)\n",
    "\n",
    "# Extracting LandScapes data by 'BioRep'\n",
    "df_gb = df_gb_dict['BioRep']\n",
    "\n",
    "# Plotting with ggplot\n",
    "Fig4b = (ggplot(df_gb, aes(\n",
    "                           x='Form',\n",
    "                           y='Îm (Da)',\n",
    "                           color='Modification',\n",
    "                           size='PrSMs',\n",
    "                           )\n",
    "                )\n",
    "         + ggtitle(f'Proteoform landscape (by biological replicate)')\n",
    "         + geom_point(alpha=1)\n",
    "         + facet_wrap('BioRep', ncol=2)\n",
    "         + theme_bw()\n",
    "         + theme(axis_text_x=element_text(angle=90))\n",
    "         + theme(\n",
    "                 legend_position=\"right\",\n",
    "                 legend_key_size=7,\n",
    "                 legend_title = element_text(size=8),\n",
    "                 legend_text = element_text(size=6)\n",
    "                 )\n",
    "         )\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig4b.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig4b.png', verbose=False)\n",
    "Fig4b.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig4b.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we will reproduce **Figure 4c** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (5, 3)\n",
    "p9.options.figure_size = (6.0, 3.6)\n",
    "\n",
    "# Extracting LandScapes data by 'TechRep'\n",
    "df_gb = df_gb_dict['TechRep']\n",
    "\n",
    "# Plotting with ggplot\n",
    "Fig4c = (ggplot(df_gb, aes(\n",
    "                           x='Form',\n",
    "                           y='Îm (Da)',\n",
    "                           color='Modification',\n",
    "                           size='PrSMs',\n",
    "                           )\n",
    "             )\n",
    "         + ggtitle(f'Proteoform landscape (by technical replicate)')\n",
    "         + geom_point(alpha=1)\n",
    "         + facet_wrap('TechRep', ncol=4)\n",
    "         + theme_bw()\n",
    "         + theme(axis_text_x=element_text(angle=90))\n",
    "         + theme(\n",
    "                 legend_position=\"right\",\n",
    "                 legend_key_size=7,\n",
    "                 legend_title=element_text(size=8),\n",
    "                 legend_text=element_text(size=6)\n",
    "                 )\n",
    "         )\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig4c.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig4c.png', verbose=False)\n",
    "Fig4c.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig4c.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Fig4a, Fig4b, Fig4c, df_gb, df_gb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 16 - Ratio plots preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clustering, we have determined the proteoform associated to each cluster. All those proteoform belong to just three protamine \"families\", which are are P1, P2-immature and  P2-mature. Since we want to monitor P1/P2-mature' and P2-immature/P2-mature ratios, we first need to include such designation into our PrSMs DataFrame `df` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating mapping dictionary to assign distinc ratio protamine families\n",
    "p1p2_dict = {\n",
    "             # P1 related proteoforms\n",
    "             'P1':          'P1',\n",
    "             # P2 related proteoforms\n",
    "             'pre-P2':      'P2-immature',\n",
    "             'HPI2':        'P2-immature',\n",
    "             'HPS1':        'P2-immature',\n",
    "             'HPS2':        'P2-immature',\n",
    "             'HP4':         'P2-mature',\n",
    "             'HP2':         'P2-mature',\n",
    "             'HP3':         'P2-mature',\n",
    "             # P2-2 related proteoforms\n",
    "             'P2-2':        'P2-immature',\n",
    "             }\n",
    "\n",
    "# Mapping revised cluster proteins with their corresponding \"Mother Proteins\"\n",
    "df['Protamine'] = df['Cluster - Protein (Revised)'].map(p1p2_dict)\n",
    "\n",
    "del p1p2_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will drop *unclassified* PrSMs (you can revisit this [code cell](#Cell_b) to recall this *classified* / *unclassified* categorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering in classified PrSMs for incomming LandScape plots\n",
    "c_Î¼ = df['Outcome'] == 'Classified'\n",
    "df_ratio = df[c_Î¼].copy()\n",
    "\n",
    "del c_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can obtain the intensity associated to each protamine \"family\" just by summing individual PrSM intensity by `'File'` and `'Node'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting and aggregating the linear Intensity by 'Replicate' and 'Engine'\n",
    "pvt_ratio = pd.pivot_table(df_ratio,\n",
    "                           values='Int',\n",
    "                           index=['File', 'Node'],\n",
    "                           columns=['Protamine'],\n",
    "                           aggfunc=sum)\n",
    "\n",
    "del df_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform summed intensities to $log_2$ to then compute the two ratios we want ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the summed linear intensities into log2\n",
    "pvt_ratio = pvt_ratio.apply(np.log2)\n",
    "\n",
    "# Extracting series for incomming (Protamine) ratio computations\n",
    "P1 = pvt_ratio['P1']\n",
    "P2_m = pvt_ratio['P2-mature']\n",
    "P2_im = pvt_ratio['P2-immature']\n",
    "\n",
    "# Computing Protamine ratios\n",
    "pvt_ratio['P1/P2-mature'] = P1 / P2_m\n",
    "pvt_ratio['P2-immature/P2-mature'] = P2_im / P2_m\n",
    "\n",
    "# Resetting index to recover associated 'Replicate' and 'Node'\n",
    "pvt_ratio.reset_index(drop=False, inplace=True)\n",
    "\n",
    "del P1, P2_im, P2_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can again extract the experimental design from the spectrum file names stored in the `'File'` column (you can revisit this [code cell](#Cell_c) to recall this step). Remember that we have two biological replicates (`'A'` and `'B'`) with two technical replicates each (`'R01'`, `'R02'` and `'R03'`, `'R04'`, respectively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the experimental desing from the file name\n",
    "for k in sorted(e.keys(), reverse=True):\n",
    "    # Incorporating the experimental desing as categorical columns\n",
    "    pvt_ratio.insert(0, e[k], pvt_ratio['File'].str.split('_').str[k])\n",
    "\n",
    "del k\n",
    "\n",
    "# Dropping futile 'File' column\n",
    "pvt_ratio.drop(columns=['File'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to monitor P1/P2-mature' and P2-immature/P2-mature ratio at all levels (Global, BioRep and TechRep), we need to add a mock `'Global'` column. After that we should melt the pivoted ratio DataFrame `pvt_ratio` in order to get a tidy format suitable for `ggplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a mock 'Global' column\n",
    "pvt_ratio['Global'] = ''\n",
    "\n",
    "# Melting both 'Ratio' 'Value's ('P1/P2-mature' and 'P2-immature/P2-mature')\n",
    "melt_ratio = pvt_ratio.melt(id_vars=['Global', 'BioRep', 'TechRep', 'Node'],\n",
    "                            value_vars=['P1/P2-mature', 'P2-immature/P2-mature'],\n",
    "                            var_name='Ratio',\n",
    "                            value_name='Value')\n",
    "\n",
    "del pvt_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to compute the `'mean'` of each `'Node'`, `'Ratio' ` pair at all levels (`'Global'`, `'BioRep'` and `'TechRep'`). Here we will also compute the up and down bounds as `'mean'` Â± `'std'` for the `'Global'` level. We need to do this by hand in order to show error bars in the following protamine ratio plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing means of each 'Node', 'Ratio' pair for all levels\n",
    "melt_ratio['Global (mean)'] = melt_ratio.groupby(by=['Global', 'Node', 'Ratio'])['Value'].transform('mean')\n",
    "melt_ratio['BioRep (mean)'] = melt_ratio.groupby(by=['BioRep', 'Node', 'Ratio'])['Value'].transform('mean')\n",
    "melt_ratio['TechRep (mean)'] = melt_ratio.groupby(by=['TechRep', 'Node', 'Ratio'])['Value'].transform('mean')\n",
    "\n",
    "# Computing standard deviations of each 'Node', 'Ratio' pair for all levels\n",
    "melt_ratio['Global (std)'] = melt_ratio.groupby(by=['Global', 'Node', 'Ratio'])['Value'].transform('std')\n",
    "\n",
    "# Computing up bounds of each 'Node', 'Ratio' pair for all levels\n",
    "melt_ratio['Global (up)'] = melt_ratio['Global (mean)'] + melt_ratio['Global (std)']\n",
    "\n",
    "# Computing down bounds of each 'Node', 'Ratio' pair for all levels\n",
    "melt_ratio['Global (down)'] = melt_ratio['Global (mean)'] - melt_ratio['Global (std)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 17 - Reproducing Figure 4 (Ratios)\n",
    "\n",
    "Here we will reproduce **Figure 4d** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (3, 3)\n",
    "p9.options.figure_size = (3.6, 3.6)\n",
    "\n",
    "# Plotting with ggplot\n",
    "Fig4d = (ggplot(melt_ratio.drop_duplicates(subset=['Global (mean)']),\n",
    "                aes(\n",
    "                    x='Global',\n",
    "                    y='Global (mean)',\n",
    "                    color='Node',\n",
    "                    fill='Node',\n",
    "                    )\n",
    "                 )\n",
    "         + geom_col(\n",
    "                    position=position_dodge2(width=0.9, preserve=\"single\"),\n",
    "                    alpha=0.5,\n",
    "                    )\n",
    "         + geom_errorbar(\n",
    "                         aes(\n",
    "                             ymin='Global (down)',\n",
    "                             ymax='Global (up)',\n",
    "                             ),\n",
    "                         width=0.9,\n",
    "                         position=position_dodge2(preserve=\"single\")\n",
    "                         )\n",
    "         + ylab('Ratio')\n",
    "         + xlab('Global')\n",
    "         + ylim(0, 1.3)\n",
    "         + facet_wrap('Ratio', ncol=1)\n",
    "         + theme_bw()\n",
    "         + theme(legend_position=\"top\")\n",
    "         )\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig4d.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig4d.png', verbose=False)\n",
    "Fig4d.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig4d.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig4d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will reproduce **Figure 4e** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (3, 3)\n",
    "p9.options.figure_size = (3.6, 3.6)\n",
    "\n",
    "# Plotting with ggplot\n",
    "Fig4e = (ggplot(melt_ratio.drop_duplicates(subset=['BioRep (mean)']),\n",
    "                aes(\n",
    "                    x='BioRep',\n",
    "                    y='BioRep (mean)',\n",
    "                    color='Node',\n",
    "                    fill='Node'\n",
    "                    )\n",
    "                )\n",
    "         + geom_col(\n",
    "                    position=position_dodge2(width=0.9, preserve=\"single\"),\n",
    "                    alpha=0.5,\n",
    "                    )\n",
    "         + ylab('Ratio')\n",
    "         + xlab('Biological replicate')\n",
    "         + ylim(0, 1.3)\n",
    "         + facet_wrap('Ratio', ncol=1)\n",
    "         + theme_bw()\n",
    "         + theme(legend_position=\"top\")\n",
    "         )\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig4e.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig4e.png', verbose=False)\n",
    "Fig4e.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig4e.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig4e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we will reproduce **Figure 4f** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (3, 3)\n",
    "p9.options.figure_size = (3.6, 3.6)\n",
    "\n",
    "# Plotting with ggplot\n",
    "Fig4f = (ggplot(melt_ratio.drop_duplicates(subset=['TechRep (mean)']),\n",
    "                aes(\n",
    "                    x='TechRep',\n",
    "                    y='TechRep (mean)',\n",
    "                    color='Node',\n",
    "                    fill='Node',\n",
    "                    )\n",
    "                 )\n",
    "         + geom_col(\n",
    "                    position=position_dodge2(width=0.9, preserve=\"single\"),\n",
    "                    alpha=0.5,\n",
    "                    )\n",
    "         + ylab('Ratio')\n",
    "         + xlab('Technical replicate')\n",
    "         + ylim(0, 1.3)\n",
    "         + facet_wrap('Ratio', ncol=1)\n",
    "         + theme_bw()\n",
    "         + theme(legend_position=\"top\")\n",
    "         )\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig4f.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig4f.png', verbose=False)\n",
    "Fig4f.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig4f.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Fig4d, Fig4e, Fig4f, melt_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 18 - PTMs localization plots preparation\n",
    "\n",
    "We will restrict the PTM localization analysis just to those PS/TP phosphorylations that according to our clustering approach doesn't appear mixed up with the +(61) Da PTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking those PrSMs with the 'phospho' or 'Phospho' keywords\n",
    "ph_Î¼ = df['Modifications'].str.contains('|'.join(['phospho', 'Phospho']))\n",
    "# (According to PD/TP)\n",
    "\n",
    "# Masking those PrSMs with the the +(61) Da PTM\n",
    "Zn_Î¼ = df['Cluster - Modification (Revised)'].str.contains('61')\n",
    "# (According to our clustering approach)\n",
    "\n",
    "# Filtlering-in those PrSMs with the 'phospho' or 'Phospho' keywords\n",
    "df_ph = df[ph_Î¼ & ~Zn_Î¼].copy()\n",
    "\n",
    "del ph_Î¼, Zn_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preliminary step to generate the PTMs localization plots, we need to get an \"extended\" version of the PrSMs DataFrame with the information regarding the PTM localization in separated rows. We didn't found an elegant clean way to achieve this, so we apologize in advance for the cumbersomeness of the next code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list with the columns that DO contain lists\n",
    "YesStack_cols = ['Targets', 'Positions', 'Positions (abs.)', 'Probabilities']\n",
    "\n",
    "# Creating a list with the columns that DO NOT contain lists\n",
    "NonStack_cols = [col for col in df_ph.columns if col not in YesStack_cols]\n",
    "\n",
    "# Computing the length of the longest list in the 'Targets' column (for the incoming stack)\n",
    "longest = df_ph['Targets'].map(len).max()\n",
    "\n",
    "# Initializing an empty dataframe for incoming extending\n",
    "df_ph_stack = pd.DataFrame({})\n",
    "\n",
    "# Extending each cell in df_ph (for the incoming stack)\n",
    "for col in NonStack_cols:\n",
    "\n",
    "    # Replacing each cell by a 'longest'-times repeated list (for the incoming stack)\n",
    "    df_ph_stack[col] = df_ph[col].apply(lambda cell: [cell] * longest)\n",
    "\n",
    "del longest, col, NonStack_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `df_ph_stack` is a very bulky DataFrame with the sole purpose of being \"stacked\" in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporating the columns that DO contain lists from df_ph to df_ph_stack\n",
    "df_ph_stack[YesStack_cols] = df_ph[YesStack_cols]\n",
    "\n",
    "# Initializing empty dataframe for incoming stacking\n",
    "df_site = pd.DataFrame({})\n",
    "\n",
    "# Stacking the lists inside each df_ph_stack colum\n",
    "for col in df_ph_stack.columns:\n",
    "    \n",
    "    # Stacking the lists inside each df_ph_stack colum\n",
    "    df_site[col] = df_ph_stack[col].apply(pd.Series).stack()\n",
    "\n",
    "del col, df_ph_stack, YesStack_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In turn, the `df_site` is a redundant DataFrame. We need to remove filling and duplicated instances in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting index\n",
    "df_site.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Renaming columns because now we have a single \"Target\" by row\n",
    "df_site.rename(columns={\n",
    "                        'Targets': 'Target',\n",
    "                        'Positions': 'Position',\n",
    "                        'Positions (abs.)': 'Position (abs.)',\n",
    "                        'Probabilities': 'Probability',\n",
    "                        },\n",
    "               inplace=True)\n",
    "\n",
    "# Masking non-filling entries\n",
    "NonFilling_Î¼ = df_site['Target'] != ''\n",
    "\n",
    "# Filtering-in non-filling entries\n",
    "df_site = df_site[NonFilling_Î¼].copy()\n",
    "\n",
    "# Dropping duplicated entries\n",
    "df_site.dropna(subset=['Target'], inplace=True)\n",
    "\n",
    "del NonFilling_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the quality filters stablished in **Part 0 - Preamble**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking PS/TP\n",
    "PS_Î¼ = df_site['Node'] == 'PS'\n",
    "TP_Î¼ = df_site['Node'] == 'TP'\n",
    "\n",
    "# Masking PS/TP best candidate sites\n",
    "TP_ok_Î¼ = df_site['Probability'] > p['PTM_TP_prob']\n",
    "PS_ok_Î¼ = df_site['C-Score'] > p['PTM_PS_CScore']\n",
    "\n",
    "# Filtering-in PS/TP best candidate sites\n",
    "df_site_f = df_site[( (TP_Î¼ & TP_ok_Î¼) | (PS_Î¼ & PS_ok_Î¼) )].copy()\n",
    "\n",
    "del TP_Î¼, TP_ok_Î¼, PS_Î¼, PS_ok_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, we have nice `'Position (abs.)'` and `'Target'` columns. Starting from this column pair we will construct a extra `'Site'` column just for aesthetics reasons when plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohercing integer datatpype\n",
    "df_site_f['Position (abs.)'] = df_site_f['Position (abs.)'].astype('int')\n",
    "\n",
    "# Storing 'Target' as Series\n",
    "target_ser = df_site_f['Target']\n",
    "\n",
    "# Cohercing string datatpype and storing 'Position (abs.)' as Series\n",
    "position_ser = df_site_f['Position (abs.)'].astype('str')\n",
    "\n",
    "# Creating the 'Site' column by hand\n",
    "df_site_f['Site'] = target_ser + '-' + position_ser\n",
    "\n",
    "del target_ser, position_ser, df_site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to select only those sites for which the associated PS/TP form matches the clustering form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing supporting UDF\n",
    "def match1(f):\n",
    "    '''TODO'''\n",
    "    return f['Cluster - Protein (Revised)'] in f['Proteins (own search)']\n",
    "\n",
    "# Masking sites with matching PS/TP and clustering forms\n",
    "match1_Î¼ = df_site_f.apply(match1, axis=1)\n",
    "\n",
    "# Filtering-in matching sites\n",
    "df_site_m1 = df_site_f[match1_Î¼].copy()\n",
    "\n",
    "# Creating a column with the whole 'Cluster - Protein (Revised)' sequence associated to the PrSM sequence\n",
    "df_site_m1['Full seq.'] = df_site_m1['Cluster - Protein (Revised)'].map(pf_seq_dict)\n",
    "\n",
    "del df_site_f, match1_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we also need to select those sites for which the associated PS/TP target residue matches the clustering target residue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing supporting UDF\n",
    "def match2(f):\n",
    "    '''TODO'''\n",
    "    try:\n",
    "        return f['Full seq.'][f['Position (abs.)'] - 2]\n",
    "    except:\n",
    "        IndexError\n",
    "\n",
    "# Getting the target residue according our clustering assignation\n",
    "df_site_m1['Cluster target'] = df_site_m1.apply(match2, axis=1)\n",
    "\n",
    "# Masking sites with matching PS/TP and clustering target residues\n",
    "match2_Î¼ = df_site_m1['Target'] == df_site_m1['Cluster target']\n",
    "\n",
    "# Filtering-in matching sites\n",
    "df_site_m2 = df_site_m1[match2_Î¼].copy()\n",
    "\n",
    "del df_site_m1, match2_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for a given PrSM, we must check if the number of phosphorylations according PS/TP match the number of phosphorylations according to our clustering approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Î»-function to count the number of phosphorylations according to our clustering approach\n",
    "ph_Î» = lambda c: re.findall(r'\\+(.*)ph', c)\n",
    "\n",
    "# Counting the number of phosphorylations according to our clustering approach\n",
    "df_site_m2['Cluster - Count (ph)'] = df_site_m2['Cluster - Modification (Revised)'].apply(ph_Î»)\n",
    "\n",
    "# Cleaning the lists number of phosphorylations to get number of phosphorylations\n",
    "df_site_m2['Cluster - Count (ph)'] = df_site_m2['Cluster - Count (ph)'].str[0].fillna(0).apply(int)\n",
    "\n",
    "# Masking sites with matching PS/TP and clustering number of phosphorylations\n",
    "match3_Î¼ = df_site_m2['Cluster - Count (ph)'] == df_site_m2['Count (ph)']\n",
    "\n",
    "# Filtering-in matching sites\n",
    "df_site_m3 = df_site_m2[match3_Î¼].copy()\n",
    "\n",
    "# Exporting the main DataFrame\n",
    "df_site_m3.to_excel(f'{os.getcwd()}\\\\Output\\\\5_Phospho.xlsx', index=False)\n",
    "\n",
    "del df_site_m2, match3_Î¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 19 - Reproducing Figure 5\n",
    "\n",
    "Finally, we will groupby in order to count the number of PTM counts broken down by all grouping items in `gb_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list for incomming groupby\n",
    "gb_list = [\n",
    "           'Position (abs.)',\n",
    "           'Node',\n",
    "           'Cluster - Protein (Revised)',\n",
    "           'Cluster - Proteoform (Revised)',\n",
    "           'Cluster - Modification (Revised)',\n",
    "           'Site',\n",
    "           ]\n",
    "\n",
    "# Grouping by aggregating by count\n",
    "df_site_g = df_site_m3.groupby(by=gb_list, as_index=False).agg({'Scan': 'count'})\n",
    "\n",
    "del gb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define the categorical order for the `'Site'` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a sorted Series with the unique sites in our dataset\n",
    "category_sites = df_site_g[['Position (abs.)', 'Site']].drop_duplicates(subset=['Site']).sort_values(by=['Position (abs.)'])\n",
    "\n",
    "# Defining 'Site' as a sorted cetogorical variable \n",
    "df_site_g['Site'] = pd.Categorical(\n",
    "                                   values=df_site_g['Site'],\n",
    "                                   categories=category_sites['Site'],\n",
    "                                   ordered=True,\n",
    "                                   )\n",
    "\n",
    "del category_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will reproduce **Figure 5b** just as appears in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the figure size 1.2 x (4, 3)\n",
    "p9.options.figure_size = (4.8, 3.6)\n",
    "\n",
    "# Plotting with ggplot\n",
    "Fig5b = (ggplot(df_site_g,\n",
    "                aes(\n",
    "                    x='Site',\n",
    "                    y='Scan',\n",
    "                    color='Cluster - Modification (Revised)',\n",
    "                    fill='Cluster - Modification (Revised)',\n",
    "                    )\n",
    "                )\n",
    "         + geom_col(\n",
    "                    position=position_dodge2(width=0.9, preserve=\"single\"),\n",
    "                    alpha=0.5,\n",
    "                    )\n",
    "         + theme_bw()\n",
    "         + theme(legend_position=\"top\", legend_title=element_blank())\n",
    "         + theme(axis_text_x=element_text(\n",
    "                                          ha='top',\n",
    "                                          angle=90,\n",
    "                                          vjust=1.0,\n",
    "                                          hjust=0.5,\n",
    "                                          )\n",
    "                 )\n",
    "         + facet_grid(['Node', 'Cluster - Protein (Revised)'], scales=\"free\")\n",
    "         + ylab('Count')\n",
    "         )\n",
    "\n",
    "# Saving our ggplot\n",
    "Fig5b.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\png\\\\Fig5b.png', verbose=False)\n",
    "Fig5b.save(filename=f'{os.getcwd()}\\\\Output\\\\Figures\\\\pdf\\\\Fig5b.pdf', verbose=False)\n",
    "\n",
    "# Showing our ggplot\n",
    "Fig5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Fig5b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
